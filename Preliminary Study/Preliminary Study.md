In our preliminary study, we aimed to evaluate the potential of using Large Language Models (LLMs) to detect WCAG success criterion violations, starting directly from the raw HTML source code of web pages. We selected five university homepages for this investigation. 

**Prompting Strategies:**
We experimented with several prompting strategies to guide the LLM in identifying accessibility violations:

**1.HTML Input with Open-Ended Prompt:**

- Prompt Template: 

​	*"Analyze the following HTML code for any WCAG 2.2 success criterion violations and list them."*

**2.HTML Input with Demonstration:**

- Prompt Template 1:

  *"Here is an example of a failure for SC 1.3.1: a table is formatted using white space characters.* 

  *White space formatted table: table element…*

   *Reason for Failure: ...* 

  *Analyze the following HTML code and identify any WCAG 2.2 violations."*

- Prompt Template 2:

  *"Here are all examples of failures in SC 1.3.1.*

  *1. A table is formatted using white space characters:*
  *White space formatted table…*

   *Reason for Failure: ...* 

  *2.An element with an invalid ARIA attribute:*
  *Element with invalid ARIA attribute…*

   *Reason for Failure: ...* 

  *...*

  *Analyze the following HTML code and identify any WCAG 2.2 violations."*

**3. Chain-of-Thought Prompting:**

- Prompt Template:
  
    *1.List each relevant WCAG 2.2 success criterion that applies to the provided HTML code.*
    
    *2.For each criterion, perform a detailed, step-by-step evaluation:*
    
    - *i. Explain what the criterion requires.*
    - *ii. Examine the HTML code and identify specific elements or attributes that relate to the criterion.*
    - *iii. Assess whether the code meets the criterion based on your explanation in Step i.*
    - *iv. Identify any violations.*



**Executions:**

For each homepage, we ran each prompting template three times to account for variability in the LLM's responses caused by its stochastic nature. To ensure reliability in our findings, we focused on identifying the most common issues across the three runs for each template. In total, we conducted 60 runs (5 homepages × 4 prompting templates × 3 iterations).



**Criteria for Success:**

A prompt was considered successful if the LLM met the following criteria:

1. Correctly identified actual WCAG violations present on the webpage.
2. Detected multiple violation types (a violation type refers to a success criterion with a violation). Based on our pre-experiment findings, existing tools typically report 4 violation types. Therefore, the threshold is set at more than 5 to ensure better performance compared to existing tools.
3. Provided responses within a reasonable time frame.

To evaluate success, we manually reviewed the results generated by the LLM (GPT-4o).



**Results:**

Please note that for the preliminary experiment with the demonstration templates, we tested SC 1.1.1 (Non-text Content), SC 1.3.1 (Info and Relationships), and SC 4.1.2 (Name, Role, Value).

The experiment was conducted based on the portion of tokens that can fit into the context window of GPT-4o.

The table below shows the average results for each prompting strategy.

| **Template**          | **TP** | **FP** | **Precision** | **Coverage** | **Response Time (seconds)** |
| --------------------- | ------ | ------ | ------------- | ------------ | --------------------------- |
| **Open-Ended Prompt** | **8**  | **58** | **12%**       | **3**        | **19**                      |
| **Demonstration 1**   | **11** | **81** | **12%**       | **3**        | **38**                      |
| **Demonstration 2**   | **14** | **65** | **18%**       | **3**        | **53**                      |
| **Chain of Thoughts** | **31** | **73** | **30%**       | **7**        | **124**                     |

**Challenges Identified:**

1. Token Limitations and Overhead When Processing Raw HTML

*Regardless of the prompting strategy, the raw HTML source code often exceeded the token limits of current LLMs, even after attempts to truncate or summarize the content.* 



2. Missing Context from External Resources When Processing Raw HTML

*Critical styling and functional information defined in external CSS and JavaScript files were unavailable in the HTML input. Attributes such as background images and font properties were inaccessible to the LLM, leading to incomplete assessments and reduced accuracy.*



3. Hallucinations When Processing Raw HTML

*The LLM frequently reported violations that were not present in the actual web pages. For instance, without access to images included as background, it incorrectly flagged elements with class names containing “background” as violations of SC 1.1.1. This success criterion involves ensuring that important images conveying information are not included as background images.*



4. Disproportionate Focus on Specific Success Criteria in Open-Ended Prompts

*The LLM consistently emphasized certain success criteria, particularly "SC 1.3.1 Info and Relationships" and "SC 4.1.2 Name, Role, Value."* 



5. Limitations of the Demonstration Prompting Strategy

*Including examples for all possible failures consumed excessive token space, reducing the model’s ability to analyze the input effectively, as the examples occupied a large portion of the context window. Conversely, providing only one specific failure to save token space caused the LLM to focus too narrowly on that example, limiting its ability to detect other types of violations.*



6. Ineffectiveness of Chain-of-Thought Prompts

*While chain-of-thought prompting encouraged a step-by-step analysis, it did not improve precision significantly. For example, the LLM sometimes considered irrelevant success criteria, such as applying SC 2.4.4 Link Purpose (In Context) to static, non-interactive text elements, like plain headings or paragraphs, where links were not present. Moreover, this strategy increased verbosity, and response times were significantly longer than those of other prompting strategies—552% longer than the open-ended prompt for the portion that fits within the context window. Given the large size of modern web pages, such delays are impractical.* 



**Conclusion**

Our preliminary study revealed that directly inputting raw HTML into the LLM, even with varied prompting strategies, was insufficient for accurate and comprehensive accessibility evaluations. The challenges identified underscored the need for a more refined approach.

The first four challenges motivated the developments of:

- **Element Extraction:** By pre-processing the web pages to extract relevant elements for each success criterion, we could provide the LLM with focused and context-rich inputs.
- **Targeted Prompts:** Crafting prompts that align closely with specific success criteria can help the LLM generate more accurate assessments.

The fifth challenge, related to the demonstration, revealed its unsuitability for use in our study.

We also tested the chain-of-thought (COT) approach after the initial implementation of element extraction. In addition to Contextual Information (CI), as described in Section 4.2.2, we incorporated the COT approach. We tested it on SC 1.1.1 (Non-text Content), SC 1.3.1 (Info and Relationships), and SC 4.1.2 (Name, Role, Value).

| **Strategy** | **TP** | **FP** | **Precision** | **Response Time (seconds)** |
| ------------ | ------ | ------ | ------------- | --------------------------- |
| **CI**       | **17** | **5**  | **77%**       | **26**                      |
| **CI+COT**   | **19** | **5**  | **79%**       | **93**                      |

Again, the precision achieved by incorporating the COT approach did not increase significantly, but the response time was 258% longer. The response time is expected to increase further when tested with additional success criteria and more complex websites. Therefore, we decided not to adopt the COT approach.